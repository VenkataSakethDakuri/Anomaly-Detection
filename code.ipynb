{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/content/modified_output.csv')\n",
    "\n",
    "# **1. Create 'Person_ID' Column**\n",
    "\n",
    "# Calculate the total number of rows per individual\n",
    "rows_per_individual = 4500\n",
    "\n",
    "# Assign 'Person_ID' based on row indices\n",
    "df['Person_ID'] = (df.index // rows_per_individual) + 1\n",
    "\n",
    "# Verify that 'Person_ID' ranges from 1 to 16\n",
    "print(f\"Unique Person_IDs: {df['Person_ID'].unique()}\")\n",
    "\n",
    "\n",
    "\n",
    "df['Health_Status'] = np.where(df['Person_ID'] <= 12, 'Healthy', 'Patient')\n",
    "\n",
    "# **3. Randomly Split Individuals into Training and Testing Sets**\n",
    "\n",
    "# Get lists of healthy and patient IDs\n",
    "healthy_ids = df.loc[df['Health_Status'] == 'Healthy', 'Person_ID'].unique()\n",
    "patient_ids = df.loc[df['Health_Status'] == 'Patient', 'Person_ID'].unique()\n",
    "\n",
    "# Randomly select 9 healthy IDs for training\n",
    "train_healthy_ids = np.random.choice(healthy_ids, size=12, replace=False)\n",
    "# The remaining healthy IDs for testing\n",
    "test_healthy_ids = np.setdiff1d(healthy_ids, train_healthy_ids)\n",
    "\n",
    "# Randomly select 3 patient IDs for training\n",
    "train_patient_ids = np.random.choice(patient_ids, size=0, replace=False)\n",
    "# The remaining patient IDs for testing\n",
    "test_patient_ids = np.setdiff1d(patient_ids, train_patient_ids)\n",
    "\n",
    "# Combine IDs for training and testing\n",
    "train_ids = np.concatenate([train_healthy_ids, train_patient_ids])\n",
    "test_ids = np.concatenate([test_healthy_ids, test_patient_ids])\n",
    "\n",
    "print(f\"Training Healthy IDs: {train_healthy_ids}\")\n",
    "print(f\"Training Patient IDs: {train_patient_ids}\")\n",
    "print(f\"Testing Healthy IDs: {test_healthy_ids}\")\n",
    "print(f\"Testing Patient IDs: {test_patient_ids}\")\n",
    "\n",
    "# Create masks for training and testing data\n",
    "train_mask = df['Person_ID'].isin(train_ids)\n",
    "test_mask = df['Person_ID'].isin(test_ids)\n",
    "\n",
    "# **4. Define Feature Columns and Target Variables**\n",
    "\n",
    "relevant_columns = [\"Body_Temperature\", \"Heart_Rate_Sensor1\",\n",
    "                    \"Heart_Rate_Sensor2\", \"SpO2\", \"ECG\"]\n",
    "\n",
    "# **Feature Engineering for Heart Rate**\n",
    "\n",
    "# 1. Average Heart Rate\n",
    "df['Average_Heart_Rate'] = df[['Heart_Rate_Sensor1', 'Heart_Rate_Sensor2']].mean(axis=1)\n",
    "\n",
    "# 2. Difference Between Heart Rate Sensors\n",
    "df['Heart_Rate_Difference'] = abs(df['Heart_Rate_Sensor1'] - df['Heart_Rate_Sensor2'])\n",
    "\n",
    "# **5. Add Rolling Mean and Std. Dev. for ECG per Individual**\n",
    "\n",
    "# Choose a rolling window size (e.g., window_size = 5)\n",
    "window_size = 5\n",
    "\n",
    "# Compute rolling statistics per individual\n",
    "df['ECG_Rolling_Mean'] = df.groupby('Person_ID')['ECG'].rolling(window=window_size, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "df['ECG_Rolling_Std'] = df.groupby('Person_ID')['ECG'].rolling(window=window_size, min_periods=1).std().reset_index(level=0, drop=True)\n",
    "\n",
    "# Handle NaN values resulting from rolling std (first few entries)\n",
    "df['ECG_Rolling_Std'] = df['ECG_Rolling_Std'].bfill()\n",
    "\n",
    "\n",
    "# **Update the list of relevant columns to include new features**\n",
    "\n",
    "engineered_features = ['Average_Heart_Rate', 'Heart_Rate_Difference', 'ECG_Rolling_Mean', 'ECG_Rolling_Std']\n",
    "relevant_columns_extended = relevant_columns + engineered_features\n",
    "\n",
    "# **6. Define Features and Target Variables**\n",
    "\n",
    "X = df[relevant_columns_extended].copy()\n",
    "Y_anomaly = df[\"anomaly\"].copy()\n",
    "Y_point = df[\"first\"].copy()\n",
    "Y_contextual = df[\"second\"].copy()\n",
    "\n",
    "# **Create Training and Testing Datasets Based on Individuals**\n",
    "\n",
    "X_train = X[train_mask].reset_index(drop=True)\n",
    "X_test = X[test_mask].reset_index(drop=True)\n",
    "\n",
    "Y_train = Y_anomaly[train_mask].reset_index(drop=True)\n",
    "Y_test = Y_anomaly[test_mask].reset_index(drop=True)\n",
    "\n",
    "Y_point_train = Y_point[train_mask].reset_index(drop=True)\n",
    "Y_point_test = Y_point[test_mask].reset_index(drop=True)\n",
    "\n",
    "Y_contextual_train = Y_contextual[train_mask].reset_index(drop=True)\n",
    "Y_contextual_test = Y_contextual[test_mask].reset_index(drop=True)\n",
    "\n",
    "# **7. Scale the Data**\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train),\n",
    "    columns=relevant_columns_extended\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test),\n",
    "    columns=relevant_columns_extended\n",
    ")\n",
    "\n",
    "# **8. Function to Detect Anomalies (Both Point and Contextual)**\n",
    "\n",
    "def detect_anomalies(X_train_scaled, X_test_scaled, svm_predictions, relevant_columns, engineered_features):\n",
    "    X_test_adjusted = X_test_scaled.copy()\n",
    "    X_test_adjusted['Initial_Prediction'] = svm_predictions\n",
    "    X_test_adjusted['Anomaly_Type'] = 0  # Initialize as normal\n",
    "\n",
    "    # Indices predicted as anomalies by SVM\n",
    "    anomaly_indices = X_test_adjusted[X_test_adjusted['Initial_Prediction'] == 1].index\n",
    "\n",
    "    # Initialize baseline residuals and feature models for contextual anomalies\n",
    "    baseline_residuals = {}\n",
    "    feature_models = {}\n",
    "\n",
    "    # Combine original and engineered features for regression\n",
    "    all_features = relevant_columns + engineered_features\n",
    "\n",
    "    # Train SVR models for each feature\n",
    "    for feature in all_features:\n",
    "        regression_features = [f for f in all_features if f != feature]\n",
    "        reg = SVR(kernel='rbf')\n",
    "        reg.fit(X_train_scaled[regression_features], X_train_scaled[feature])\n",
    "        feature_models[feature] = reg\n",
    "\n",
    "        # Calculate baseline residuals\n",
    "        train_predictions = reg.predict(X_train_scaled[regression_features])\n",
    "        residuals = np.abs(train_predictions - X_train_scaled[feature])\n",
    "        baseline_residuals[feature] = np.percentile(residuals, 95)\n",
    "\n",
    "    # Now detect point anomalies using thresholds within the function\n",
    "    # Apply thresholds for each feature\n",
    "    # Note: We need to access the original (unscaled) test data\n",
    "\n",
    "    # Use the original test data (unscaled)\n",
    "    X_test_original = X_test.copy()\n",
    "\n",
    "    # Heart Rate Sensors\n",
    "    hr_threshold_low = 55\n",
    "    hr_threshold_high = 110\n",
    "    hr1_anomalies = (X_test_original['Heart_Rate_Sensor1'] < hr_threshold_low) | (X_test_original['Heart_Rate_Sensor1'] > hr_threshold_high)\n",
    "    hr2_anomalies = (X_test_original['Heart_Rate_Sensor2'] < hr_threshold_low) | (X_test_original['Heart_Rate_Sensor2'] > hr_threshold_high)\n",
    "\n",
    "    # Body Temperature\n",
    "    temp_threshold_low = 93\n",
    "    temp_threshold_high = 110\n",
    "    temp_anomalies = (X_test_original['Body_Temperature'] < temp_threshold_low) | (X_test_original['Body_Temperature'] > temp_threshold_high)\n",
    "\n",
    "    # SpO2\n",
    "    spo2_threshold_low = 92\n",
    "    spo2_threshold_high = 100\n",
    "    spo2_anomalies = (X_test_original['SpO2'] < spo2_threshold_low) | (X_test_original['SpO2'] > spo2_threshold_high)\n",
    "\n",
    "    # ECG\n",
    "    ecg_threshold_low = 450\n",
    "    ecg_threshold_high = 700\n",
    "    ecg_anomalies = (X_test_original['ECG'] < ecg_threshold_low) | (X_test_original['ECG'] > ecg_threshold_high)\n",
    "\n",
    "    # Combine all point anomalies\n",
    "    point_anomalies_indices = X_test_original.index[hr1_anomalies | hr2_anomalies | temp_anomalies | spo2_anomalies | ecg_anomalies]\n",
    "\n",
    "    # Set 'Anomaly_Type' for point anomalies\n",
    "    X_test_adjusted.loc[point_anomalies_indices, 'Anomaly_Type'] = 1  # Point anomalies\n",
    "\n",
    "    # Remaining indices to check for contextual anomalies\n",
    "    remaining_indices = anomaly_indices.difference(point_anomalies_indices)\n",
    "\n",
    "    # Detect contextual anomalies among the remaining anomalies predicted by SVM\n",
    "    for idx in remaining_indices:\n",
    "        feature_violations = []\n",
    "        context_scores = []\n",
    "\n",
    "        for feature in all_features:\n",
    "            regression_features = [f for f in all_features if f != feature]\n",
    "            model = feature_models[feature]\n",
    "\n",
    "            predicted_value = model.predict(X_test_scaled.loc[[idx], regression_features])[0]\n",
    "            actual_value = X_test_scaled.loc[idx, feature]\n",
    "            residual_error = abs(predicted_value - actual_value)\n",
    "\n",
    "            if residual_error > baseline_residuals[feature]:\n",
    "                feature_violations.append(feature)\n",
    "                context_scores.append(residual_error / baseline_residuals[feature])\n",
    "\n",
    "        if len(feature_violations) >= 2:\n",
    "            X_test_adjusted.loc[idx, 'Anomaly_Type'] = 2  # Contextual anomaly\n",
    "        else:\n",
    "            # If not a contextual anomaly, and not already marked as point anomaly, mark as point anomaly\n",
    "            X_test_adjusted.loc[idx, 'Anomaly_Type'] = 1  # Point anomaly\n",
    "\n",
    "    return X_test_adjusted\n",
    "\n",
    "# **9. Train the SVM Model**\n",
    "\n",
    "svm_model = SVC(kernel='rbf', random_state=42)\n",
    "svm_model.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# **10. Get Initial SVM Predictions**\n",
    "\n",
    "svm_predictions = svm_model.predict(X_test_scaled)\n",
    "\n",
    "# **11. Detect Anomalies Using SVM Predictions, Thresholds, and SVR-Based Contextual Detection**\n",
    "\n",
    "results = detect_anomalies(\n",
    "    X_train_scaled, X_test_scaled, svm_predictions, relevant_columns, engineered_features\n",
    ")\n",
    "\n",
    "# **12. Get Final Predictions**\n",
    "\n",
    "final_point_predictions = (results['Anomaly_Type'] == 1).astype(int)\n",
    "final_contextual_predictions = (results['Anomaly_Type'] == 2).astype(int)\n",
    "final_overall_predictions = (final_point_predictions | final_contextual_predictions).astype(int)\n",
    "\n",
    "# **13. Calculate Accuracies**\n",
    "\n",
    "accuracy_overall = accuracy_score(Y_test, final_overall_predictions)\n",
    "accuracy_point = accuracy_score(Y_point_test, final_point_predictions)\n",
    "accuracy_contextual = accuracy_score(Y_contextual_test, final_contextual_predictions)\n",
    "\n",
    "# **14. Print Accuracies**\n",
    "\n",
    "print(\"\\nAccuracy Scores:\")\n",
    "print(f\"Overall Anomaly Detection Accuracy: {accuracy_overall:.4f}\")\n",
    "print(f\"Point Anomaly Detection Accuracy: {accuracy_point:.4f}\")\n",
    "print(f\"Contextual Anomaly Detection Accuracy: {accuracy_contextual:.4f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
